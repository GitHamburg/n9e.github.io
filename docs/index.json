[
  {
    "content": " 采用Docker Compose做编排，用于单机快速启动环境做测试，包含了MySQL、Redis、Prometheus、Ibex、Nightingale、Telegraf\n 从Github下载夜莺的源码，进入docker目录，执行docker-compose up -d即可，docker-compose会自动拉取镜像并启动，查看各个容器启动状态，使用命令docker-compose ps，都是Up状态则表示启动成功。\n如果ibex、nserver、nwebapi等模块一直在Restarting，可能是数据库容器启动太慢了没有准备好，可以执行docker-compose down停掉再重新尝试启动测试，这个问题受限于个人知识水平一直不知道如何解决，如果有对 docker-compose 机制熟悉的小伙伴可以赐教下，怎么保证各个容器的启动顺序，要求 MySQL 进程启动之后，其他的进程才启动。\n$ git clone https://github.com/didi/nightingale.git $ cd nightingale/docker $ docker-compose up -d Creating network \"docker_nightingale\" with driver \"bridge\" Creating mysql ... done Creating redis ... done Creating prometheus ... done Creating ibex ... done Creating agentd ... done Creating nwebapi ... done Creating nserver ... done Creating telegraf ... done $ docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------- agentd /app/ibex agentd Up 10090/tcp, 20090/tcp ibex /app/ibex server Up 0.0.0.0:10090-\u003e10090/tcp, 0.0.0.0:20090-\u003e20090/tcp mysql docker-entrypoint.sh mysqld Up 0.0.0.0:3306-\u003e3306/tcp, 33060/tcp nserver /app/n9e server Up 18000/tcp, 0.0.0.0:19000-\u003e19000/tcp nwebapi /app/n9e webapi Up 0.0.0.0:18000-\u003e18000/tcp, 19000/tcp prometheus /bin/prometheus --config.f ... Up 0.0.0.0:9090-\u003e9090/tcp redis docker-entrypoint.sh redis ... Up 0.0.0.0:6379-\u003e6379/tcp telegraf /entrypoint.sh telegraf Up 0.0.0.0:8092-\u003e8092/udp, 0.0.0.0:8094-\u003e8094/tcp, 0.0.0.0:8125-\u003e8125/udp 更多docker-compose相关知识请参考官网\nWarning启动成功之后，建议把initsql目录下的内容挪走，这样下次重启的时候，DB就不会重新初始化了。否则下次启动mysql还是会自动执行initsql下面的sql文件导致DB重新初始化，页面上创建的规则、用户等都会丢失。\ndocker-compose 这种部署方式，只是用于简单测试，不推荐在生产环境使用，当然了，如果您是 docker-compose 专家，另当别论。\n 服务启动之后，浏览器访问nwebapi的端口，即18000，默认用户是root，密码是root.2020\n",
    "description": "",
    "tags": null,
    "title": "使用Docker快速启动测试",
    "uri": "/quickstart/compose/"
  },
  {
    "content": " 本节讲述如何部署单机版，单机版对于很多中小公司足够用了，简单高效、快速直接，建议使用云主机，性能不够了直接升配，可以应对每秒上报的数据点小于100万的情形，如果只是监控机器（每台机器每个周期大概采集200个数据点）采集周期频率设置10秒的话，支撑上限是5万台\n 如果仅仅是为了快速测试，Docker 部署方式是最快的，不过很多朋友未必有 Docker 环境，另外为了减少引入更多技术栈，增强生产环境稳定性，有些朋友可能也不愿意用 Docker，那本篇就来讲解如何快速部署单机版，单机版的配套时序库是使用 Prometheus。如果要监控的机器有几千台，服务有几百个，单机版的容量无法满足，可以上集群版，集群版的时序库建议使用 VictoriaMetrics，也可以使用 M3DB，不过 M3DB 的架构更复杂，很多朋友无法搞定，选择简单的 VictoriaMetrics，对大部分公司来讲，足够用了。我们先来看一下服务端架构：\n按照单机版本的这个架构图可以看出，服务端需要安装的组件有：MySQL、Redis、Prometheus、n9e-server、n9e-webapi，Agent 有多种选型，可以是 Telegraf、Datadog-Agent、Grafana-Agent 等，Agent 应该部署在所有的目标机器上，包括服务端的这台机器，Exporters 是指 Prometheus 生态的各类 Exporter 采集器，比如 mysqld_exporter、redis_exporter、blackbox_exporter 等，这些 Exporter 是非必须的，看各自公司的情况。\n环境准备 依赖的组件有：mysql、redis、prometheus，这三个组件都是开源软件，请大家自行安装，其中prometheus在启动的时候要注意开启 --enable-feature=remote-write-receiver 这里也提供一个小脚本来安装这3个组件，大家可以参考：\n# install prometheus mkdir -p /opt/prometheus wget https://s3-gz01.didistatic.com/n9e-pub/prome/prometheus-2.28.0.linux-amd64.tar.gz -O prometheus-2.28.0.linux-amd64.tar.gz tar xf prometheus-2.28.0.linux-amd64.tar.gz cp -far prometheus-2.28.0.linux-amd64/* /opt/prometheus/ # service  cat \u003c\u003cEOF \u003e/etc/systemd/system/prometheus.service [Unit] Description=\"prometheus\" Documentation=https://prometheus.io/ After=network.target [Service] Type=simple ExecStart=/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml --storage.tsdb.path=/opt/prometheus/data --web.enable-lifecycle --enable-feature=remote-write-receiver --query.lookback-delta=2m Restart=on-failure SuccessExitStatus=0 LimitNOFILE=65536 StandardOutput=syslog StandardError=syslog SyslogIdentifier=prometheus [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable prometheus systemctl restart prometheus systemctl status prometheus # install mysql yum -y install mariadb* systemctl enable mariadb systemctl restart mariadb mysql -e \"SET PASSWORD FOR 'root'@'localhost' = PASSWORD('1234');\" # install redis yum install -y redis systemctl enable redis systemctl restart redis 上例中mysql的root密码设置为了1234，建议维持这个不变，后续就省去了修改配置文件的麻烦。\n安装夜莺组件 mkdir -p /opt/n9e \u0026\u0026 cd /opt/n9e # 去 https://github.com/didi/nightingale/releases 找最新版本的包，文档里的包地址可能已经不是最新的了 tarball=n9e-5.5.0.tar.gz urlpath=https://github.com/didi/nightingale/releases/download/v5.5.0/${tarball} wget $urlpath || exit 1 tar zxvf ${tarball} mysql -uroot -p1234 \u003c docker/initsql/a-n9e.sql nohup ./n9e server \u0026\u003e server.log \u0026 nohup ./n9e webapi \u0026\u003e webapi.log \u0026 # check logs # check port 如果启动成功，server 默认会监听在 19000 端口，webapi 会监听在 18000 端口，且日志没有报错。上面使用 nohup 简单演示，生产环境建议用 systemd 托管，相关 service 文件可以在 etc/service 目录下，供参考。\n配置文件etc/server.conf和etc/webapi.conf中都含有 mysql 的连接地址配置，检查一下用户名和密码，prometheus 如果使用上面的脚本安装，默认会监听本机 9090 端口，server.conf 和 webapi.conf 中的 prometheus 相关地址都不用修改就是对的。\n好了，浏览器访问 webapi 的端口（默认是18000）就可以体验相关功能了，默认用户是root，密码是root.2020。如果安装过程出现问题，可以参考公众号（云原生监控）的视频教程。\n夜莺服务端部署好了，接下来要考虑监控数据采集的问题，如果是 Prometheus 重度用户，可以继续使用各类 Exporter 来采集，只要数据进了时序库了，夜莺就能够消费（判断告警、展示图表等）；如果想快速看到效果，可以使用 Telegraf 来采集监控数据，请参考后续文档章节。\n",
    "description": "",
    "tags": null,
    "title": "使用二进制部署单机版服务端",
    "uri": "/quickstart/standalone/"
  },
  {
    "content": "如果您对Docker的使用非常熟悉，建议利用Docker compose的方式快速启动测试，请参考使用Docker Compose快速部署，如果对Docker不熟悉，那就用二进制方式部署，也非常简单，最小的可运行环境是Prometheus+MySQL+Redis+Nightingale，请参考快速在生产环境部署启动单机版。这个最小的环境只有Prometheus采集到的自身的一些监控指标，略显单薄，此时，我们可以引入Telegraf，采集机器、网络设备、各类中间件的指标，请参考使用Telegraf采集监控数据。\n如果公司体量很大，建议把单机版本的Prometheus替换为VictoriaMetrics，请参考使用VictoriaMetrics作为时序库。或者直接部署多个Prometheus，按照业务线或者按照地域来划分集群，此时你可能需要接入多个Prom/VM/M3DB集群，在引入多个TSDB的过程中，就要同步使用夜莺的多Server部署模型了，请参考生产环境部署高可用集群版\n",
    "description": "",
    "tags": null,
    "title": "安装部署",
    "uri": "/quickstart/"
  },
  {
    "content": " Telegraf 是 InfluxData 开源的一款采集器，可以采集操作系统、各种中间件的监控指标，采集目标列表，看起来是非常丰富，Telegraf是一个大一统的设计，即一个二进制可以采集CPU、内存、mysql、mongodb、redis、snmp等，不像Prometheus的exporter，每个监控对象一个exporter，管理起来略麻烦。一个二进制分发起来确实比较方便。\n 这里提供快速安装的教程，Telegraf的更多知识，请参考Telegraf官网，笔者之前也写了一个Telegraf调研笔记，讲解了Telegraf的基本用法，一定要看！！！，大家亦可参考。\nTelegraf下载地址在这里，根据自己的平台选择对应的二进制下载即可。笔者的环境是CentOS，下面是安装脚本，/opt/telegraf/telegraf.conf 是一个经过删减的干净的配置文件，指定了opentsdb output plugin，这个plugin的写入地址配置的是n9e-server，所以，Telegraf采集的数据会被推送给n9e-server，二者贯通：\n#!/bin/sh  version=1.20.4 tarball=telegraf-${version}_linux_amd64.tar.gz wget https://dl.influxdata.com/telegraf/releases/$tarball tar xzvf $tarball mkdir -p /opt/telegraf cp -far telegraf-${version}/usr/bin/telegraf /opt/telegraf cat \u003c\u003cEOF \u003e /opt/telegraf/telegraf.conf [global_tags] [agent] interval = \"10s\" round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \"0s\" flush_interval = \"10s\" flush_jitter = \"0s\" precision = \"\" hostname = \"\" omit_hostname = false [[outputs.opentsdb]] host = \"http://127.0.0.1\" port = 19000 http_batch_size = 50 http_path = \"/opentsdb/put\" debug = false separator = \"_\" [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = true [[inputs.disk]] ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] [[inputs.system]] fielddrop = [\"uptime_format\"] [[inputs.net]] ignore_protocol_stats = true EOF cat \u003c\u003cEOF \u003e /etc/systemd/system/telegraf.service [Unit] Description=\"telegraf\" After=network.target [Service] Type=simple ExecStart=/opt/telegraf/telegraf --config telegraf.conf WorkingDirectory=/opt/telegraf SuccessExitStatus=0 LimitNOFILE=65536 StandardOutput=syslog StandardError=syslog SyslogIdentifier=telegraf KillMode=process KillSignal=SIGQUIT TimeoutStopSec=5 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable telegraf systemctl restart telegraf systemctl status telegraf  Warning/opt/telegraf/telegraf.conf的内容是个删减版，只是为了让大家快速跑起来，如果要采集更多监控对象，比如mysql、redis、tomcat等，还是要仔细去阅读从tarball里解压出来的那个配置文件，那里有很详细的注释，也可以参考官方提供的各个采集插件下的README\n 💡 Telegraf 采集的 Linux 层面的监控数据，我们整理了一个告警规则配置，可以直接导入，JSON在这里 监控大盘正在整理，未来会放到 这里\n",
    "description": "",
    "tags": null,
    "title": "使用Telegraf采集监控数据",
    "uri": "/quickstart/telegraf/"
  },
  {
    "content": " VictoriaMetrics 架构简单，可靠性高，在性能，成本，可扩展性方面表现出色，社区活跃，且和 Prometheus 生态绑定紧密。夜莺推荐您在生产环境中使用 VictoriaMetrics 作为时序数据库。\n VictoriaMetrics 提供单机版和集群版。如果您的每秒写入数据点数小于100万，VictoriaMetrics 官方默认推荐您使用单机版，单机版可以通过增加服务器的CPU核心数，增加内存，增加IOPS来获得线性的性能提升。且单机版易于配置和运维。\n接下来的文章，介绍在夜莺中，以 VictoriaMetrics 集群版本作为时序数据库为例，完整的安装和配置过程。\nvmstorage、vminsert、vmselect 三者组合构成 VictoriaMetrics 的集群功能，三者都可以通过启动多个实例来分担承载流量。\n vmstorage 是数据存储模块：\n  其数据保存在-storageDataPath指定的目录中，默认为./vmstorage-data/，vmstorage 是有状态模块，删除 storage node 会丢失约 1/N的历史数据（N 为集群中 vmstorage node 的节点数量）。增加 storage node，则需要同步修改 vminsert 和 vmselect 的启动参数，将新加入的storage node节点地址通过命令行参数 -storageNode传入给vminsert和vmselect。 vmstorage 启动后，会监听三个端口，分别是 -httpListenAddr :8482、-vminsertAddr :8400、-vmselectAddr :8401。端口8400负责接收来自 vminsert 的写入请求，端口8401负责接收来自 vmselect 的数据查询请求，端口8482则是 vmstorage 自身提供的 http api 接口。   vminsert 接收来自客户端的数据写入请求，并负责转发到选定的vmstorage：\n  vminsert 接收到数据写入请求后，按照 jump consistent hash 算法，将数据转发到选定的某个vmstorage node 上。vminsert 本身是无状态模块，可以增加或者删除一个或多个实例，而不会造成数据的损失。vminsert 模块通过启动时的参数 -storageNode xxx,yyy,zzz 来感知到整个 vmstorage 集群的完整 node 地址列表。 vminsert 启动后，会监听一个端口-httpListenAddr :8480。该端口实现了 prometheus remote_write协议，因此可以接收和解析通过 remote_write 协议写入的数据。不过要注意，VictoriaMetrics 集群版本具有多租户功能，因此租户ID会以如下形式出现在 API URL 中: http://vminsert:8480/insert/\u003caccount_id\u003e/prometheus/api/v1/write。 更多 URL Format 可以参考 VictoriaMetrics官网。   vmselect 接收来自客户端的数据查询请求，并负责转发到所有的 vmstorage 查询结果并合并：\n  vmselect 启动后，会监听一个端口-httpListenAddr :8481。该端口实现了 prometheus remote_query等协议，因此可以接收和解析 remote_query 协议的查询。不过要注意，VictoriaMetrics 集群版本具有多租户功能，因此租户ID会以如下形式出现在 API URL 中: http://vminsert:8481/select/\u003caccount_id\u003e/prometheus/api/v1/query。 更多 URL Format 可以参考 VictoriaMetrics官网。  下载和安装 VictoriaMetrics 集群版  去 vm release 下载编译好的二进制版本，比如我们选择下载 v1.69.0 amd64。 解压缩后得到：  $ls -l vm*-prod -rwxr-xr-x 1 work work 10946416 Nov 8 22:03 vminsert-prod* -rwxr-xr-x 1 work work 13000624 Nov 8 22:03 vmselect-prod* -rwxr-xr-x 1 work work 11476736 Nov 8 22:03 vmstorage-prod*  启动三个 vmstorage 实例(可以用下面的脚本快速生成不同实例的启动命令)：  #!/bin/bash  for i in `seq 0 2`; do if [ $i -eq 0 ]; then i=\"\" fi pp=$i httpListenAddr=${pp}8482 vminsertAddr=${pp}8400 vmselectAddr=${pp}8401 storageDataPath=./${pp}vmstorage-data prog=\"nohup ./vmstorage-prod -loggerTimezone Asia/Shanghai \\ -storageDataPath $storageDataPath\\ -httpListenAddr :$httpListenAddr\\ -vminsertAddr :$vminsertAddr\\ -vmselectAddr :$vmselectAddr\\ \u0026\u003e ${pp}vmstor.log \u0026\" echo $prog (exec \"$prog\") done 也可以输入以下命令行启动三个实例：\nnohup ./vmstorage-prod -loggerTimezone Asia/Shanghai -storageDataPath ./vmstorage-data -httpListenAddr :8482 -vminsertAddr :8400 -vmselectAddr :8401 \u0026\u003e vmstor.log \u0026 nohup ./vmstorage-prod -loggerTimezone Asia/Shanghai -storageDataPath ./1vmstorage-data -httpListenAddr :18482 -vminsertAddr :18400 -vmselectAddr :18401 \u0026\u003e 1vmstor.log \u0026 nohup ./vmstorage-prod -loggerTimezone Asia/Shanghai -storageDataPath ./2vmstorage-data -httpListenAddr :28482 -vminsertAddr :28400 -vmselectAddr :28401 \u0026\u003e 2vmstor.log \u0026  启动一个 vminsert 实例：  nohup ./vminsert-prod -httpListenAddr :8480 -storageNode=127.0.0.1:8400,127.0.0.1:18400,127.0.0.1:28400 \u0026\u003evminsert.log \u0026  启动一个 vmselect 实例：  nohup ./vmselect-prod -httpListenAddr :8481 -storageNode=127.0.0.1:8401,127.0.0.1:18401,127.0.0.1:28401 \u0026\u003evmselect.log \u0026  查看 vmstorage，vminsert，vmselect 的 /metrics 接口:  curl http://127.0.0.1:8482/metrics curl http://127.0.0.1:18482/metrics curl http://127.0.0.1:28482/metrics curl http://127.0.0.1:8481/metrics curl http://127.0.0.1:8480/metrics  n9e-server通过remote write接口写入时序库，vm作为时序库的一个选择，其remote write接口地址为：http://127.0.0.1:8480/insert/0/prometheus/api/v1/write 把这个地址配置到server.conf当中即可，配置完了重启n9e-server  # Reader部分修改Url [Reader] Url = \"http://172.21.0.8:8481/select/0/prometheus\" # Writers部分修改Url [[Writers]] Url = \"http://172.21.0.8:8480/insert/0/prometheus/api/v1/write\"  修改您的 n9e-webapi 的配置文件 ./etc/webapi.conf 如下：  [[Clusters]] # Prometheus cluster name Name = \"Default\" # Prometheus APIs base url Prom = \"http://127.0.0.1:8481/select/0/prometheus\" 然后，重新启动n9e-webapi，这样夜莺就可以通过 remote query 查询到 victoriametrics 集群的数据了。\nInfon9e-webapi 的安装、配置和启动，请参考 这里。\n FAQ  VictoriaMetrics 单机版本如何保障数据的可靠性？  vm 针对磁盘IO有针对性的优化，单机版可以考虑将数据的可靠性保障交给 EBS 等云盘来保证。\n  VictoriaMetrics 如何评估容量？  参考vm的官方文档。\n  VictoriaMetrics 集群版本增加或者删除vmstorage Node的时候，数据如何再平衡？  vm 不支持扩缩容节点时，对数据进行自动的再平衡。\n  VictoriaMetrics 的数据大小如何查看？  可以通过 vmstorage 实例暴露的 /metrics 接口来获取到相应的统计数据，譬如：\n $ curl http://127.0.0.1:8482/metrics |grep -i data_size vm_data_size_bytes{type=\"indexdb\"} 609291 vm_data_size_bytes{type=\"storage/big\"} 0 vm_data_size_bytes{type=\"storage/small\"} 8749893  vminsert 在将数据写入多个 vmstorage Node的时候，是按照什么规则将数据写入到不同的 node 上的？  采用jump consistent hash 对数据进行分片，写入到相应的storage node上。\n  vmselect 在接到查询请求的时候，如何定位到请求的数据是在哪个 storage node上的？  vmselect 并不知道每个metrics对应的数据分布的storage node，vmselect会对所有的storage node发起查询请求，最后进行数据合并，并返回。\n  VictoriaMetrics 和 M3db 的对比和选择？  m3db架构设计上更高级，实现难度高，m3db在时序数据功能之后，重点解决了自动扩缩容，数据自动平衡等运维难题。但是因此也更复杂，可靠性目前也更难保证。VictoriaMetrics架构设计上的tradeoff 更倾向于简单可靠，重点优化了单机版的性能，强调垂直扩展，同时和prometheus 生态做到兼容，甚至于在很多的点上做到了加强。但是 VictoriaMetrics 对于时序数据downsample，节点的自动扩缩容，数据自动再平衡等高级功能和分布式能力，是有缺失的。\n   相关资料  使用 Docker Compose 快速部署 VictoriaMetrics。 使用 Helm Chart 快速在 Kubernetes中部署 VictoriaMetrics。 使用 VictoriaMetrics Operator 在 Kubernetes中部署 VictoriaMetrics。 VictoriaMetrics 集群版架构：   ",
    "description": "",
    "tags": null,
    "title": "使用VictoriaMetrics作为时序库",
    "uri": "/quickstart/victoriametrics/"
  },
  {
    "content": "由于Prometheus没有集群版本，受限于容量问题，很多公司会搭建多套Prometheus，比如按照业务拆分，不同的业务使用不同的Prometheus集群，或者按照地域拆分，不同的地域使用不同的Prometheus集群。这里是以Prometheus来举例，VictoriaMetrics、M3DB都有集群版本，不过有时为了不相互干扰和地域网络问题，也会拆成多个集群。对于多集群的协同，需要在夜莺里做一些配置，回顾一下夜莺的架构图：\n图上分了 3 个 region，每个 region 一套时序库，每个 region 一套 n9e-server，n9e-server 依赖 redis，所以每个 region 一个 redis，n9e-webapi 和 mysql 放到中心，n9e-webapi 也依赖一个 redis，所以中心端放置的是 n9e-webapi、redis、mysql，如果想图省事，redis 也是可以复用的，各个 region 的 n9e-server 都连接中心的 redis 也是可以的。\n为了高可用，各个 region 的 n9e-server 可以多部署几个实例组成一个集群，集群中的所有 n9e-server 的配置文件 server.conf 中的 ClusterName 要设置成一样的字符串。\n假设，我们有两个时序库，在北京搭建了一个 Prometheus，在广州搭建了一个 VictoriaMetrics，n9e-webapi 会把这两个时序库作为 DataSource，所以在 n9e-webapi 的配置文件中，要配置上这俩存储的地址，举例：\n[[Clusters]] # cluster name Name = \"Prom-Beijing\" # Prometheus APIs base url Prom = \"http://10.2.3.4:9090\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 [[Clusters]] # cluster name Name = \"VM-Guangzhou\" # Prometheus APIs base url Prom = \"http://172.21.0.8:8481/select/0/prometheus\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 另外图上也可以看出，一个 n9e-server 对应一个时序库，所以在 n9e-server 的配置文件中，也需要配置对应的时序库的地址，比如北京的 server，配置如下，Writers 下面的 Url 配置的是 remote write 的地址，而 Reader 下面配置的 Url 是实现Prometheus 原生查询接口的 BaseUrl。\n[Reader] # prometheus base url Url = \"http://127.0.0.1:9090\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 10 [[Writers]] Url = \"http://127.0.0.1:9090/api/v1/write\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 假设上海区域用的是 VictoriaMetrics，所以 Url 略有不同，配置如下：\n[Reader] # prometheus base url Url = \"http://127.0.0.1:8481/select/0/prometheus\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 10 [[Writers]] Url = \"http://127.0.0.1:8480/insert/0/prometheus/api/v1/write\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 n9e-webapi 是要响应前端 ajax 请求的，前端会从 n9e-webapi 查询监控数据，n9e-webapi 自身不存储监控数据，而是仅仅做了一个代理，把请求代理给后端的时序库，前端读取数据时会调用 Prometheus 的那些原生接口，即：/api/v1/query /api/v1/query_range /api/v1/labels 这种接口，所以注意啦，n9e-webapi 中配置的 Clusters 下面的Url，都是要支持Prometheus 原生接口的 BaseUrl。\n对于 n9e-server，有两个重要作用，一个是接收监控数据，然后转发给后端多个 Writer，所以，Writer 可以配置多个，配置文件是 toml 格式，[[Writers]]双中括号这种就表示数组，数据写给后端存储，走的协议是 Prometheus 的 Remote Write，所以，所有支持 Remote Write 的存储，都可以使用。n9e-server 的另一个重要作用，是做告警判断，会周期性从 mysql 同步告警规则，然后根据用户配置的 PromQL 调用时序库的 query 接口，所以 n9e-server 的 Reader 下面的 Url，也是要配置支持 Prometheus 原生接口的 BaseUrl。另外注意，Writer 可以配置多个，但是 Reader 只能配置一个。比如监控数据可以写一份到Prometheus 存储近期数据用于告警判断，再写一份到 OpenTSDB 存储长期数据，Writer 就可以配置为 Prometheus 和 OpenTSDB 这两个，而 Reader 只配置 Prometheus 即可。\n",
    "description": "",
    "tags": null,
    "title": "接入多个Prom/VM/M3DB集群",
    "uri": "/quickstart/multitsdb/"
  },
  {
    "content": "对于规模相对较小的公司，比如几百台机器这个体量，个人认为单机版足够用了，使用云主机部署，性能不足可以直接升配，存储使用云存储保证，硬件故障云平台也会自动把虚拟机热迁移走，非常省心。那如果咱们体量确实比较大，或者没有云主机这种基础设施，这里会讲解集群版的部署方式。\n时序库 时序库的集群部署，就看时序库自身的机制了，这里不展开，如果是使用 Prometheus，Prometheus 没有集群版，VictoriaMetrics、M3DB、Thanos 等都是有集群版的，请参考他们的文档\nMySQL MySQL 一般部署成主从，这个请你们的 DBA 来搞吧，或者直接使用云上 RDS，这里就不展开了\nRedis 夜莺当前 n9e-webapi 和 n9e-server 都依赖 redis，redis 一般有三种模式，单机模式、集群模式、哨兵模式，夜莺当前只支持单机模式。具体使用几个 redis，要看大家的环境情况，如果图省事，全局就使用一个 redis 也是可以的。也可以把 n9e-webapi 用的 redis 和 n9e-server 用的 redis 分开，可以参考文档：接入多个 Prom/VM/M3DB 集群\nn9e-webapi 这个模块是放中心的，可以部署多个实例，前面统一放置 nginx 或者 lvs，某个 n9e-webapi 实例如果挂了，nginx、lvs 都可以自动摘除，保证了高可用。\nn9e-server n9e-server 是随着时序库走的，每个时序库对应一套 n9e-server，一套 n9e-server 可以只有一个 n9e-server 实例，也可以部署多个保证高可用和性能。同一套 n9e-server 内部的多个实例，其配置文件 server.conf 中的 ClusterName 字段，要配置成一样的字符串。\n",
    "description": "",
    "tags": null,
    "title": "生产环境部署高可用集群版",
    "uri": "/quickstart/clusters/"
  },
  {
    "content": "首页有个架构图，大家可以看到，夜莺把接收到的监控数据都直接写入了后端时序数据库，所以，读取监控数据，无需经由夜莺的接口，直接读取后端的时序库的接口就可以了。即：如果使用了 Prometheus，就通过 Prometheus 的接口读取监控数据，如果用了 VictoriaMetrics，就通过 VictoriaMetrics 的接口读取监控数据。\n比如 Prometheus，就是那些/api/v1/query /api/v1/query_range之类的接口。相关接口文档请参考：Prometheus官网\n",
    "description": "",
    "tags": null,
    "title": "读取监控数据",
    "uri": "/api/read/"
  },
  {
    "content": "概述 所谓的告警自愈，典型手段是在告警触发时自动回调某个webhook地址，在这个webhook里写告警自愈的逻辑，夜莺默认支持这种方式。另外，夜莺还可以更进一步，配合ibex这个模块，在告警触发的时候，自动去告警的机器执行某个脚本，这种机制可以大幅简化构建运维自愈链路的工作量，毕竟，不是所有的运维人员都擅长写http server，但所有的运维人员，都擅长写脚本。这种方式是典型的物理机时代的产物，希望各位朋友用不到这个工具（说明贵司的IT技术已经走得非常靠前了）。\n架构 ibex模块，类似之前夜莺v3版本中的job模块，可以批量执行脚本，其架构非常简单，包括server和agentd两个模块，agentd周期性调用server的rpc接口，询问有哪些任务要执行，如果有分配给自己的任务，就从server拿到任务脚本信息，在本地fork一个进程运行，然后将结果上报给服务端。为了简化部署，server和agentd融合成了一个二进制，就是ibex，通过传入不同的参数来启动不同的角色。ibex架构图如下：\n项目地址  Git仓库：https://gitee.com/cnperl/ibex 编译方法看这里 Linux 下编译好的包 在这里  安装启动 下载安装包之后，解压缩，在etc下可以找到服务端和客户端的配置文件，在sql目录下可以找到初始化sql脚本。\n初始化sql mysql \u003c sql/ibex.sql 启动server server的配置文件是etc/server.conf，注意修改里边的mysql连接地址，配置正确的mysql用户名和密码。然后就可以直接启动了：\nnohup ./ibex server \u0026\u003e server.log \u0026 ibex没有web页面，只提供api接口，鉴权方式是http basic auth，basic auth的用户名和密码默认都是ibex，在etc/server.conf中可以找到，如果ibex部署在互联网，一定要修改默认用户名和密码，当然，因为n9e要调用ibex，所以n9e的server.conf和webapi.conf中也配置了ibex的basic auth账号信息，要改就要一起改啦。\n启动agentd 客户端的配置非常非常简单，agentd.conf内容如下：\n# debug, release RunMode = \"release\" # task meta storage dir MetaDir = \"./meta\" [HTTP] Enable = true # http listening address Host = \"0.0.0.0\" # http listening port Port = 2090 # https cert file path CertFile = \"\" # https key file path KeyFile = \"\" # whether print access log PrintAccessLog = true # whether enable pprof PProf = false # http graceful shutdown timeout, unit: s ShutdownTimeout = 30 # max content length: 64M MaxContentLength = 67108864 # http server read timeout, unit: s ReadTimeout = 20 # http server write timeout, unit: s WriteTimeout = 40 # http server idle timeout, unit: s IdleTimeout = 120 [Heartbeat] # unit: ms Interval = 1000 # rpc servers Servers = [\"10.2.3.4:20090\"] # $ip or $hostname or specified string Host = \"telegraf01\" 客户端的HTTP接口用处不大，可以把Enable设置为false，关闭监听，重点关注Heartbeat这个部分，Interval是心跳频率，默认是1000毫秒，如果机器量比较小，比如小于1000台，维持1000没问题，如果机器量比较大，可以适当调大这个频率，比如2000或者3000，可以减轻服务端的压力。Servers是个数组，配置的是ibex-server的地址，ibex-server可以启动多个，多个地址都配置到这里即可，Host这个字段，是本机的唯一标识，有三种配置方式，如果配置为$ip，系统会自动探测本机的IP，如果是$hostname，系统会自动探测本机的hostname，如果是其他字符串，那就直接把该字符串作为本机的唯一标识。每个机器上都要部署ibex-agentd，不同的机器要保证Host字段获取的内容不能重复。\n另外，Telegraf的配置文件中，有下面这么一段：\n[agent] interval = \"10s\" round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \"0s\" flush_interval = \"10s\" flush_jitter = \"0s\" precision = \"\" hostname = \"\" omit_hostname = false 其中hostname默认留空，表示自动探测本机的hostname，如果写了具体的某个字符串，那就把写的那个字符串作为监控数据的host字段的内容，这个hostname字段要和ibex的agentd.conf中的Host字段保持一致，典型的做法有：\n Telegraf中把hostname配置为空，Telegraf自动获取本机hostname，ibex的Host配置为$hostname，ibex也会自动获取本机hostname，这样Telegraf和ibex可以获取到相同的标识内容 Telegraf中手工把hostname配置为本机的ip，ibex则把Host配置为$ip，这样二者也可以获取到相同的标识内容 Telegraf和ibex都使用某个特定写死的字符串来作为标识信息，这样也OK，但是要保证不同的机器，这个字符串不能重复  下面是启动ibex-agentd的命令：\nnohup ./ibex agentd \u0026\u003e agentd.log \u0026 另外，细心的读者应该会发现ibex的压缩包里的etc目录下有个service目录，里边准备好了两个service样例文件，便于大家使用systemd来管理ibex进程，生产环境，建议使用systemd来管理。\n",
    "description": "",
    "tags": null,
    "title": "告警自愈依赖的脚本下发执行模块",
    "uri": "/quickstart/ibex/"
  },
  {
    "content": "调用 n9e-server 的 /opentsdb/put 接口，POST 方法，该接口实现了 OpenTSDB 的数据协议，监控数据做成 JSON 放到 HTTP Request Body 中，举例：\n[ { \"metric\": \"cpu_usage_idle\", \"timestamp\": 1637732157, \"tags\": { \"cpu\": \"cpu-total\", \"ident\": \"c3-ceph01.bj\" }, \"value\": 30.5 }, { \"metric\": \"cpu_usage_util\", \"timestamp\": 1637732157, \"tags\": { \"cpu\": \"cpu-total\", \"ident\": \"c3-ceph01.bj\" }, \"value\": 69.5 } ] 显然，JSON 最外层是个数组，如果只上报一条监控数据，也可以不要外面的中括号，直接把对象结构上报：\n{ \"metric\": \"cpu_usage_idle\", \"timestamp\": 1637732157, \"tags\": { \"cpu\": \"cpu-total\", \"ident\": \"c3-ceph01.bj\" }, \"value\": 30.5 } 服务端会看第一个字符是否是[，来判断上报的是数组，还是单个对象，自动做相应的 Decode。如果觉得上报的内容太过占用带宽，也可以做 gzip 压缩，此时上报的数据，要带有Content-Encoding: gzip的 Header。\nInfo注意 ident 这个标签，ident 是 identity 的缩写，表示设备的唯一标识，如果标签中有 ident 标签，n9e-server 就认为这个监控数据是来自某个机器的，会自动获取 ident 的 value，注册到监控对象的列表里\n ",
    "description": "",
    "tags": null,
    "title": "推送监控数据（OpenTSDB协议）",
    "uri": "/api/opentsdb/"
  },
  {
    "content": " 本节讲述 Nightingale 的源码编译方式，分前后端两部分。另外，如果用到告警自愈模块，会用到 ibex 这个模块，本节也会一并讲解 ibex 模块的编译方法。对于 ARM 的处理器，我们没有提供编译好的二进制，大家就要用下面的方法自行编译了。\n 前端 git clone https://github.com/n9e/fe-v5.git cd fe-v5 npm install npm run build 后端 Nightingale 后端采用 Go 语言编写，编译的前置条件就是配置 Go 的开发环境。\n配置Go环境 到Go官网选择对应的版本下载，我的环境是Linux，选择的go1.17.3.linux-amd64.tar.gz，直接下载到/root目录下了，然后解压缩，即Go的内容都放到了/root/go目录下了。同时准备gopath目录，如下：\ncd /root \u0026\u0026 mkdir -p gopath/src echo \"GOROOT=/root/go\" \u003e\u003e .bash_profile echo \"GOPATH=/root/gopath\" \u003e\u003e .bash_profile echo 'export PATH=$GOROOT/bin:$GOPATH/bin:$PATH' \u003e\u003e .bash_profile source .bash_profile 编译n9e git clone https://github.com/didi/nightingale.git # 国内配置一下代理，可以加速编译 export GOPROXY=https://goproxy.cn # 执行编译 cd nightingale \u0026\u0026 make 编译完成之后如果生成二进制：n9e，就表示编译成功！想要快速入门Go语言？可以参考GOCN的资料！\n编译ibex 如果需要告警自愈能力，夜莺依赖ibex做命令下发执行，ibex的编译和n9e几乎一模一样，如下：\ngit clone https://gitee.com/cnperl/ibex.git # 国内配置一下代理，可以加速编译 export GOPROXY=https://goproxy.cn # 执行编译 cd ibex \u0026\u0026 make 编译完成之后如果生成二进制：ibex，就表示编译成功！\n",
    "description": "",
    "tags": null,
    "title": "源码编译夜莺前后端及告警自愈模块",
    "uri": "/quickstart/compile/"
  },
  {
    "content": "简介 n9e-webapi 模块提供了两类接口，一个是 /api/n9e 打头的，给前端调用，另一类是 /v1/n9e 打头的，给第三方系统调用。如果想以个人身份模仿WEB操作，也是调用 /api/n9e 相关接口。\n以个人身份模仿WEB操作 这种方式，页面上 JavaScript 可以调用的所有接口，你都可以用程序调用，打开 chrome 的开发者工具，扒拉这些接口，还是非常容易的。当然，要先登录，登录调用 webapi 模块的 /api/n9e/auth/login 接口，系统使用 jwt 认证，如果登录成功，会返回 access_token 和 refresh_token，每次调用的时候都要把 access_token 放到 Header 里，access_token 差不多15分钟过期，之后可以重新调用登录接口换 token，也可以调用 /api/n9e/auth/refresh 接口用 refresh_token 换一个新的 access_token，当然，也会顺道返回一个新的 refresh_token，举例：\n# 调用登录接口拿到access_token和refresh_token记录下来，后面调用其他接口的时候会用到 [root@10-255-0-34 ~]# curl -X POST 'http://localhost:18000/api/n9e/auth/login' -d '{\"username\": \"root\", \"password\": \"root.2020\"}' {\"dat\":{\"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCIsImF1dGhvcml6ZWQiOnRydWUsImV4cCI6MTYzNzgyMzA1OSwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.nJ56Pc7qS5Ik_UaVmlNWu_QlABaBc4pZ_WkU45u2wWk\",\"refresh_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzgzMzc4NTksInJlZnJlc2hfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCsrMS1yb290IiwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.JKsbfTYBCOOfR_oPsf496N9ml9yXbP7BHb4E8Yfnzbo\",\"user\":{\"id\":1,\"username\":\"root\",\"nickname\":\"超管\",\"phone\":\"\",\"email\":\"\",\"portrait\":\"\",\"roles\":[\"Admin\"],\"contacts\":{},\"create_at\":1637545881,\"create_by\":\"system\",\"update_at\":1637546351,\"update_by\":\"root\",\"admin\":true}},\"err\":\"\"} # access_token放到Authorization这个Header里，Bearer的验证方式 [root@10-255-0-34 ~]# curl -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCIsImF1dGhvcml6ZWQiOnRydWUsImV4cCI6MTYzNzgyMzA1OSwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.nJ56Pc7qS5Ik_UaVmlNWu_QlABaBc4pZ_WkU45u2wWk\" 'http://localhost:18000/api/n9e/self/profile' {\"dat\":{\"id\":1,\"username\":\"root\",\"nickname\":\"超管\",\"phone\":\"\",\"email\":\"\",\"portrait\":\"\",\"roles\":[\"Admin\"],\"contacts\":{},\"create_at\":1637545881,\"create_by\":\"system\",\"update_at\":1637546351,\"update_by\":\"root\",\"admin\":true},\"err\":\"\"} # 如果token过期了，后端会返回异常HTTP状态码，此时要调用refresh接口换取新的token [root@10-255-0-34 ~]# curl -X POST 'http://localhost:18000/api/n9e/auth/refresh' -d '{\"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzgzMzc4NTksInJlZnJlc2hfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCsrMS1yb290IiwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.JKsbfTYBCOOfR_oPsf496N9ml9yXbP7BHb4E8Yfnzbo\"}' {\"dat\":{\"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NfdXVpZCI6IjAxMzkzYzkxLTk5MWItNGE0Yi04ODk2LTJhZGRjMDUwYjcxMCIsImF1dGhvcml6ZWQiOnRydWUsImV4cCI6MTYzNzgyMzMxOCwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.2BeWyYfcnRi3qw69zecaaeFnPFUNAGsiPIZBBnd5lug\",\"refresh_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzgzMzgxMTgsInJlZnJlc2hfdXVpZCI6IjAxMzkzYzkxLTk5MWItNGE0Yi04ODk2LTJhZGRjMDUwYjcxMCsrMS1yb290IiwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.zFZaRYcJI6G5maSgDVF-jZzxQ3Tb5dybIqufJhBy034\"},\"err\":\"\"} 第三方系统调用夜莺 比如第三方系统想获取夜莺中的所有未恢复告警，或者获取夜莺中的全量用户列表，这些需求，建议走 /v1/n9e 打头的接口，这些接口走 BasicAuth 认证，BasicAuth 的用户名和密码在 webapi.conf 中可以找到，就是 BasicAuth 那个 section 的配置。当前这个阶段，/v1/n9e 前缀的接口还比较少，不过代码框架已经搭起来了，代码在 src/webapi/router/router.go 文件中，如果贵司要封装夜莺的接口，可能要在这个路由分组下加一些路由配置了。作为开源软件，说清楚原理就好了，如果贵司仍然搞不明白可以联系我们，我们提供商业技术支持服务 :-)\n",
    "description": "",
    "tags": null,
    "title": "调用webapi的接口",
    "uri": "/api/webapi/"
  },
  {
    "content": "夜莺无需对接 Grafana，夜莺会把监控数据转存到后端时序库，比如 Prometheus、VictoriaMetrics、M3DB 等，把这些时序库配置为 Grafana 的数据源即可。\n",
    "description": "",
    "tags": null,
    "title": "对接Grafana",
    "uri": "/usage/grafana/"
  },
  {
    "content": "监控 Linux 常用的有两种方式，一个是通过部署 Telegraf，一个是通过 node_exporter，关注三个层面的问题：怎么部署？配置哪些告警规则？监控大盘如何配置？\nTelegraf  部署方式：使用Telegraf采集监控数据 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/linux_by_telegraf.json 监控大盘：正在整理  node_exporter  部署方式：https://github.com/prometheus/node_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/node_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/node_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控Linux",
    "uri": "/usage/linux/"
  },
  {
    "content": "推荐使用 windows_exporter 监控 windows\n 部署方式：https://github.com/prometheus-community/windows_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/windows_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/windows_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控Windows",
    "uri": "/usage/windows/"
  },
  {
    "content": "Telegraf内置支持snmp的采集，本节给一个入门例子，让大家快速上手，更多具体知识可以参考这里。在telegraf.conf中搜索inputs.snmp，即可找到对应的配置，例子如下：\n[[inputs.snmp]] agents = [\"udp://172.25.79.194:161\"] timeout = \"5s\" version = 3 agent_host_tag = \"ident\" retries = 1 sec_name = \"managev3user\" auth_protocol = \"SHA\" auth_password = \"example.Demo.c0m\" [[inputs.snmp.field]] oid = \"RFC1213-MIB::sysUpTime.0\" name = \"uptime\" [[inputs.snmp.field]] oid = \"RFC1213-MIB::sysName.0\" name = \"source\" is_tag = true [[inputs.snmp.table]] oid = \"IF-MIB::ifTable\" name = \"interface\" inherit_tags = [\"source\"] [[inputs.snmp.table.field]] oid = \"IF-MIB::ifDescr\" name = \"ifDescr\" is_tag = true 上面非常关键的部分是：agent_host_tag = \"ident\"，因为夜莺对ident这个标签会特殊对待处理，把携有这个标签的数据当做隶属某个监控对象的数据，机器和网络设备都是典型的期望作为监控对象来管理的，所以snmp的采集中，我们把网络设备的ip放到ident这个标签里带上去。\n另外这个采集规则是v3的校验方法，不同的公司可能配置的校验方式不同，请各位参照telegraf.conf中那些snmp相关的注释仔细核对，如果是v2会简单很多，把上例中的如下部分：\nversion = 3 sec_name = \"managev3user\" auth_protocol = \"SHA\" auth_password = \"example.Demo.c0m\" 换成：\nversion = 2 community = \"public\" 即可，当然了，community要改成你们自己的，这里写的public只是举个例子。\ninputs.snmp.field相关的那些配置，可以采集到各个网口的监控指标，更多的使用方式请参考官网\n 另外，snmp的采集，建议大家使用专门的Telegraf来做，因为和机器、中间件等的采集频率可能不同，比如边缘交换机，我们5min采集一次就够了，如果按照默认的配置可是10s采集一次，实在是太频繁了，可能会给一些老式交换机造成比较大的压力，采集频率在telegraf.conf的最上面[agent]部分，边缘交换机建议配置为：\n[agent] interval = \"300s\" flush_interval = \"300s\" 核心交换机可以配置的频繁一些，比如60s或者120s，请各位网络工程师朋友自行斟酌。\n",
    "description": "",
    "tags": null,
    "title": "监控网络设备",
    "uri": "/usage/snmp/"
  },
  {
    "content": "Google提出了应用监控的4个黄金指标，分别是：流量、延迟、错误、饱和度，其中前面3个指标都可以通过内嵌SDK的方式埋点采集。夜莺核心模块有两个，webapi主要是提供http接口给JavaScript调用，server主要是负责接收监控数据，处理告警规则，这两个模块都引入了Prometheus的Go的SDK，用此方式做App Performance监控，本节以夜莺的代码为例，讲解如何使用Prometheus的SDK。\nwebapi监控 webapi模块主要统计两个内容，一个是请求的数量统计，一个是请求的延迟统计，统计时，要用不同的Label做维度区分，后面就可以通过不同的维度做多种多样的统计分析，对于HTTP请求，规划4个核心Label，分别是：service、code、path、method。service标识服务名称，要求全局唯一，便于和其他服务名称区分开，比如webapi模块，就定义为n9e-webapi，code是http返回的状态码，200就表示成功数量，其他code就是失败的，后面我们可以据此统计成功率，method是HTTP方法，GET、POST、PUT、DELETE等，比如新增用户和获取用户列表可能都是/api/n9e/users，从路径上无法区分，只能再加上method才能区分开。\npath着重说一下，表示请求路径，比如上面提到的/api/n9e/users，但是，在restful实践中，url中经常会有参数，比如获取编号为1的用户的信息，接口是/api/n9e/user/1，获取编号为2的用户信息，接口是/api/n9e/user/2，如果这俩带有用户编号的url都作为Label，会造成时序库索引爆炸，而且从业务方使用角度来看，我们也不关注编号为1的用户获取请求还是编号为2的用户获取请求，而是关注整体的GET /api/n9e/user/:id这个接口的监控数据。所以我们在设置Label的时候，要把path设置为/api/n9e/user/:id，而不是那具体的带有用户编号的url路径。夜莺用的gin框架，gin框架有个FullPath方法就是获取这个信息的，比较方便。\n首先，我们在webapi下面创建一个stat包，放置相关统计变量：\npackage stat import ( \"time\" \"github.com/prometheus/client_golang/prometheus\" ) const Service = \"n9e-webapi\" var ( labels = []string{\"service\", \"code\", \"path\", \"method\"} uptime = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"uptime\", Help: \"HTTP service uptime.\", }, []string{\"service\"}, ) RequestCounter = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"http_request_count_total\", Help: \"Total number of HTTP requests made.\", }, labels, ) RequestDuration = prometheus.NewHistogramVec( prometheus.HistogramOpts{ Buckets: []float64{.01, .1, 1, 10}, Name: \"http_request_duration_seconds\", Help: \"HTTP request latencies in seconds.\", }, labels, ) ) func Init() { // Register the summary and the histogram with Prometheus's default registry. \tprometheus.MustRegister( uptime, RequestCounter, RequestDuration, ) go recordUptime() } // recordUptime increases service uptime per second. func recordUptime() { for range time.Tick(time.Second) { uptime.WithLabelValues(Service).Inc() } } uptime变量是顺手为之，统计进程启动了多久时间，不用太关注，RequestCounter和RequestDuration，分别统计请求流量和请求延迟。Init方法是在webapi模块进程初始化的时候调用，所以进程一起，就会自动注册好。\n然后我们写一个middleware，在请求进来的时候拦截一下，省的每个请求都要去统计，middleware方法的代码如下：\nimport ( ... promstat \"github.com/didi/nightingale/v5/src/webapi/stat\" ) func stat() gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() c.Next() code := fmt.Sprintf(\"%d\", c.Writer.Status()) method := c.Request.Method labels := []string{promstat.Service, code, c.FullPath(), method} promstat.RequestCounter.WithLabelValues(labels...).Inc() promstat.RequestDuration.WithLabelValues(labels...).Observe(float64(time.Since(start).Seconds())) } } 有了这个middleware之后，new出gin的engine的时候，就立马Use一下，代码如下：\n... r := gin.New() r.Use(stat()) ... 最后，监控数据要通过/metrics接口暴露出去，我们要暴露这个请求端点，代码如下：\nimport ( ... \"github.com/prometheus/client_golang/prometheus/promhttp\" ) func configRoute(r *gin.Engine, version string) { ... r.GET(\"/metrics\", gin.WrapH(promhttp.Handler())) } 如上，每个webapi的接口的流量和成功率都可以监控到了。如果你也部署了夜莺，请求webapi的端口(默认是18000)的/metrics接口看看吧。\nInfo如果服务部署多个实例，甚至多个region，多个环境，上面的4个Label就不够用了，因为只有这4个Label不足以唯一标识一个具体的实例，此时需要env、region、instance这种Label，这些Label不需要在代码里埋点，在采集的时候一般可以附加额外的标签，通过附加标签的方式来处理即可\n server监控 server模块的监控，和webapi模块的监控差异较大，因为关注点不同，webapi关注的是HTTP接口的请求量和延迟，而server模块关注的是接收了多少监控指标，内部事件队列的长度，从数据库同步告警规则花费多久，同步了多少条数据等，所以，我们也需要在server的package下创建一个stat包，stat包下放置stat.go，内容如下：\npackage stat import ( \"github.com/prometheus/client_golang/prometheus\" ) const ( namespace = \"n9e\" subsystem = \"server\" ) var ( // 各个周期性任务的执行耗时 \tGaugeCronDuration = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"cron_duration\", Help: \"Cron method use duration, unit: ms.\", }, []string{\"cluster\", \"name\"}) // 从数据库同步数据的时候，同步的条数 \tGaugeSyncNumber = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"cron_sync_number\", Help: \"Cron sync number.\", }, []string{\"cluster\", \"name\"}) // 从各个接收接口接收到的监控数据总量 \tCounterSampleTotal = prometheus.NewCounterVec(prometheus.CounterOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"samples_received_total\", Help: \"Total number samples received.\", }, []string{\"cluster\", \"channel\"}) // 产生的告警总量 \tCounterAlertsTotal = prometheus.NewCounterVec(prometheus.CounterOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"alerts_total\", Help: \"Total number alert events.\", }, []string{\"cluster\"}) // 内存中的告警事件队列的长度 \tGaugeAlertQueueSize = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"alert_queue_size\", Help: \"The size of alert queue.\", }, []string{\"cluster\"}) ) func Init() { // Register the summary and the histogram with Prometheus's default registry. \tprometheus.MustRegister( GaugeCronDuration, GaugeSyncNumber, CounterSampleTotal, CounterAlertsTotal, GaugeAlertQueueSize, ) } 定义一个监控指标，除了name之外，还可以设置namespace、subsystem，最终通过/metrics接口暴露的时候，可以发现：监控指标的最终名字，就是$namespace_$subsystem_$name，三者拼接在一起。webapi模块的监控代码中我们看到了counter类型和histogram类型的处理，这次我们拿GaugeAlertQueueSize举例，这是个GAUGE类型的统计数据，起一个goroutine周期性获取队列长度，然后Set到GaugeAlertQueueSize中：\npackage engine import ( \"context\" \"time\" \"github.com/didi/nightingale/v5/src/server/config\" promstat \"github.com/didi/nightingale/v5/src/server/stat\" ) func Start(ctx context.Context) error { ... go reportQueueSize() return nil } func reportQueueSize() { for { time.Sleep(time.Second) promstat.GaugeAlertQueueSize.WithLabelValues(config.C.ClusterName).Set(float64(EventQueue.Len())) } } 另外，Init方法要在server模块初始化的时候调用，server的router.go中要暴露/metrics端点路径，这些就不再详述了，大家可以扒拉一下夜莺的代码看一下。\n数据抓取 应用自身的监控数据已经通过/metrics接口暴露了，后续采集规则可以在prometheus.yml中配置，prometheus.yml中有个section叫：scrape_configs可以配置抓取目标，这是Prometheus范畴的知识了，大家可以参考Prometheus官网。\n参考资料  https://prometheus.io/docs/instrumenting/clientlibs/ https://github.com/prometheus/client_golang/tree/master/examples  ",
    "description": "",
    "tags": null,
    "title": "埋点监控之PromSDK",
    "uri": "/usage/promapm/"
  },
  {
    "content": "StatsD简介 在内嵌Prometheus SDK做APM一节中，我们介绍了业务进程内嵌Prometheus的SDK做埋点，这种方式，会把监控数据聚合计算的逻辑放在业务进程中，比如一些平均值、分位值的计算，可能会对业务进程造成影响，本节要介绍的StatsD的方式，理念是把指标聚合计算的逻辑挪到业务进程之外，业务进程调用埋点函数的时候，通过UDP推送给StatsD，即使StatsD挂了，也不会对业务进程造成影响。\nStatsD的简介，网上一搜一大把，请大家自行Google，这里就不重复描述了。核心要理解一下StatsD的设计理念、协议、支持的各个语言的SDK（在附录里有）即可，下面直接拿一个小例子讲解如何利用Telegraf支持StatsD协议的数据，数据只要进了Telegraf了，就意味着可以推到夜莺了，夜莺就相当于支持了StatsD的埋点监控采集能力。\nTelegraf启用StatsD 在Telegraf的配置文件中搜索inputs.statsd就能看到对应的section：\n[[inputs.statsd]] protocol = \"udp\" service_address = \":8125\" percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0] metric_separator = \"_\" 启用如上配置，percentiles略微有点多，可以配置的少一点，比如percentiles = [50.0, 90.0, 99.0, 100.0]，这样整体计算存储压力也会小一些。重启Telegraf，Telegraf就会在8125端口监听udp协议，接收业务埋点数据的上报。即，Telegraf实现了StatsD的协议，可以作为StatsD的Server使用。\n在业务程序中埋点 附录里罗列了一些客户端SDK，这里笔者使用Go语言的一个SDK来测试，实现了一个很小的web程序，代码如下：\npackage main import ( \"fmt\" \"math/rand\" \"net/http\" \"time\" \"github.com/smira/go-statsd\" ) var client *statsd.Client func homeHandler(w http.ResponseWriter, r *http.Request) { start := time.Now() // random sleep \tnum := rand.Int31n(100) time.Sleep(time.Duration(num) * time.Millisecond) fmt.Fprintf(w, \"duration: %d\", num) client.Incr(\"requests.counter,page=home\", 1) client.PrecisionTiming(\"requests.latency,page=home\", time.Since(start)) } func main() { // init client \tclient = statsd.NewClient(\"localhost:8125\", statsd.TagStyle(statsd.TagFormatInfluxDB), statsd.MaxPacketSize(1400), statsd.MetricPrefix(\"http.\"), statsd.DefaultTags(statsd.StringTag(\"service\", \"n9e-webapi\"), statsd.StringTag(\"region\", \"bj\")), ) defer client.Close() http.HandleFunc(\"/\", homeHandler) http.ListenAndServe(\":8000\", nil) } 这个web服务只有一个根路径，逻辑也很简单，就是随机sleep几十个毫秒当做业务处理时间。整体逻辑是这样的：首先，我们要通过statsd.NewClient初始化一个statsd的客户端，参数中指定了StatsD的Server地址（在本例中就是Telegraf的8125），指定了所有监控指标的前缀是http.，还指定了两个全局Tag，一个是service=n9e-webapi，另一个是region=bj，通过TagStyle指定了要发送的是InfluxDB样式（因为数据是发给Telegraf的，Telegraf是InfluxDB生态的）的标签。然后，在请求的具体处理逻辑里上报了两个监控指标，一个是requests.counter，另一个是requests.latency，并且，为这俩指标指定了一个指标级别的标签page=home，整体看起来还是比较简单的。\n测试方法 上面的Go程序编译一下，启动，会作为一个web server监听在8000端口，然后周期性请求这个web server的地址做测试，这个web server接收到请求之后，就调用statsd的sdk，statsd的sdk的核心逻辑就是把数据发给Telegraf的8125，然后就是Telegraf处理聚合逻辑，聚合之后的数据每10s（默认flush频率）发给夜莺。\n在页面上，应该可以看到http_requests_latency和http_requests_counter打头的相关指标，比如http_requests_latency_mean这个指标，会看到这个指标有如下几个标签：\n ident: VM-0-4-centos 这个标签其实是Telegraf原始的host标签，夜莺的规范里叫ident，所以做了一下rename metric_type: timing 这个显然是把statsd的数据类型也做为标签了，其他数据类型还有gauge、counter、set等 page: home 这是我们代码里附到监控指标后面的标签，Telegraf自动帮解析出来了 service: n9e-webapi NewClient时候附加的全局默认标签 region: bj NewClient时候附加的全局默认标签  附录资料  Measure Anything, Measure Everything Statsd支持的的客户端SDK列表  ",
    "description": "",
    "tags": null,
    "title": "埋点监控之StatsdSDK",
    "uri": "/usage/statsd/"
  },
  {
    "content": "请参考如下视频教程：\n 01-使用Docker Compose一行命令安装夜莺v5 02-快速在生产环境部署启动单机版夜莺v5 03-讲解夜莺v5人员组织相关功能 04-讲解夜莺v5监控看图相关功能 05-讲解夜莺v5告警规则的使用 06-讲解夜莺v5告警屏蔽规则的使用 07-讲解夜莺告警订阅规则的使用 08-讲解夜莺v5活跃告警和历史告警 09-讲解夜莺v5告警自愈脚本的使用 10-讲解夜莺监控对象的管理功能 11-夜莺v5如何接入多个时序存储 12-讲解夜莺v5配置文件  如果仍然有使用问题，可以联系我们，联系方式如下，公众号：\n",
    "description": "",
    "tags": null,
    "title": "功能介绍",
    "uri": "/usage/"
  },
  {
    "content": "之前在写调研笔记的时候，测试了PING监控和TCP探测监控，调研笔记在 这里 这个章节主要给大家讲解域名URL探测。直接上测试配置：\n[[inputs.http_response]] urls = [\"https://www.baidu.com\", \"http://ulricqin.io/ping\"] response_timeout = \"5s\" method = \"GET\" fielddrop = [\"result_type\"] tagexclude = [\"result\", \"status_code\"] https://www.baidu.com 显然是通的，http://ulricqin.io/ping 这个是个假的URL，不通，我们测试一下输出的内容：\n[root@10-255-0-34 telegraf-1.20.3]# ./usr/bin/telegraf --config etc/telegraf/telegraf.conf --input-filter http_response --test 2021-12-13T04:16:43Z I! Starting Telegraf 1.20.3 \u003e http_response,host=10-255-0-34,method=GET,server=https://www.baidu.com content_length=227i,http_response_code=200i,response_time=0.028757521,result_code=0i 1639369003000000000 \u003e http_response,host=10-255-0-34,method=GET,server=http://ulricqin.io/ping result_code=5i 1639369003000000000 这里有个字段是result_code，用这个字段配置告警即可，正常可以访问的URL，result_code是0，不正常就是非0，告警规则里可以配置如下promql：\nhttp_response_result_code != 0 或者直接在夜莺的告警规则页面导入这条告警规则JSON：\n[ { \"name\": \"有URL探测失败，请注意\", \"note\": \"\", \"severity\": 1, \"disabled\": 0, \"prom_for_duration\": 60, \"prom_ql\": \"http_response_result_code != 0\", \"prom_eval_interval\": 15, \"enable_stime\": \"00:00\", \"enable_etime\": \"23:59\", \"enable_days_of_week\": [ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"0\" ], \"notify_recovered\": 1, \"notify_channels\": [ \"email\", \"dingtalk\", \"wecom\" ], \"notify_repeat_step\": 60, \"callbacks\": [], \"runbook_url\": \"\", \"append_tags\": [] } ] 如果想对域名返回的statuscode或者response body的内容做判断，Telegraf也是支持的，使用response_status_code和response_string_match这些字段配置，配置文件里有样例，大家可以自行参考下。\n",
    "description": "",
    "tags": null,
    "title": "监控URL",
    "uri": "/usage/http_response/"
  },
  {
    "content": "前言 说到日志监控，大家第一反应的可能是ELK的方案，或者Loki的方案，这两个方案都是把日志采集了发到中心，在中心存储、查看、分析，不过这个方案相对比较重量级一些，如果我们的需求只是从日志中提取一些metrics数据，比如统计一些日志中出现的Error次数之类的，则有一个更简单的方案。\n这个方案在夜莺v4版本中是有的，不过后来推荐大家客户端使用Telegraf，Telegraf没有这个能力，所以v5版本的夜莺没法监控日志，怎么办呢？这里给大家介绍一个Google出品的小工具，mtail，mtail和夜莺v4的方案类似，就是流式读取日志，通过正则表达式匹配的方式从日志中提取metrics指标，这种方式可以利用目标机器的算力，不过如果量太大，可能会影响目标机器上的业务程序，另外一个好处是无侵入性，不需要业务埋点，如果业务程序是第三方供应商提供的，我们改不了其代码，mtail此时就非常合适了。当然了，如果业务程序是我们公司的人自己写的，那还是建议用埋点的方式采集指标，mtail只是作为一个补充吧。\nmtail简介 mtail的使用方案，参考如下两个文档（下载的话参考Releases页面）：\n Deploying Programming Guide  我们拿mtail的启动命令来举例其用法：\nmtail --progs /etc/mtail --logs /var/log/syslog --logs /var/log/ntp/peerstats 通过 --progs 参数指定一个目录，这个目录里放置一堆的*.mtail文件，每个mtail文件就是描述的正则提取规则，通过 --logs 参数来指定要监控的日志目录，可以写通配符，--logs 可以写多次，上例中只是指定了 --progs 和 --logs ，没有其他参数，mtail启动之后会自动监听一个端口3903，在3903的/metrics接口暴露符合Prometheus协议的监控数据，Prometheus（或者Telegraf）就可以从 /metrics 接口提取监控数据。\n这样看起来，原理就很清晰了，mtail启动之后，根据 --logs 找到相关日志文件文件，seek到文件末尾，开始流式读取，每读到一行，就根据 --progs 指定的那些规则文件做匹配，看是否符合某些正则，从中提取时序数据，然后通过3903的/metrics暴露采集到的监控指标。当然，除了Prometheus这种/metrics方式暴露，mtail还支持把监控数据直接推给graphite或者statsd，具体可以参考：这里\nmtail样例 这里我用mtail监控一下n9e-server的日志，从中提取一下各个告警规则触发的notify的数量，这个日志举例：\n2021-12-27 10:00:30.537582 INFO engine/logger.go:19 event(cbb8d4be5efd07983c296aaa4dec5737 triggered) notify: rule_id=9 [__name__=net_response_result_code author=qin ident=10-255-0-34 port=4567 protocol=tcp server=localhost]2@1640570430 很明显，日志中有这么个关键字：notify: rule_id=9，可以用正则来匹配，统计出现的行数，ruleid也可以从中提取到，这样，我们可以把ruleid作为标签上报，于是乎，我们就可以写出这样的mtail规则了：\n[root@10-255-0-34 nightingale]# cat /etc/mtail/n9e-server.mtail counter mtail_alert_rule_notify_total by ruleid /notify: rule_id=(?P\u003cruleid\u003e\\d+)/ { mtail_alert_rule_notify_total[$ruleid]++ } 然后启动也比较简单，我这里就用nohup简单来做：\nnohup mtail -logtostderr --progs /etc/mtail --logs server.log \u0026\u003e stdout.log \u0026 mtail没有指定绝对路径，是因为我把mtail的二进制直接放在了 /usr/bin 下面了，mtail默认会监听在3903，所以我们可以用如下命令验证：\ncurl -s localhost:3903/metrics 可以看到输出如下内容：\n# HELP mtail_alert_rule_notify_total defined at n9e-server.mtail:1:9-37 # TYPE mtail_alert_rule_notify_total counter mtail_alert_rule_notify_total{prog=\"n9e-server.mtail\",ruleid=\"9\"} 6 上面的输出只是挑选了部分内容，没有全部放出来哈，这就表示正常采集到了，如果n9e的server.log中当前没有打印notify相关的日志，那请求/metrics接口是没法得到上面的输出的，可以手工配置一条必然会触发的规则，待日志里有相关输出的时候再次请求 /metrics 接口，应该就有了\n最后，我们使用Telegraf来采集一下 localhost:3903/metrics 这个地址的输出，在telegraf.conf中添加如下配置：\n[[inputs.prometheus]] urls = [\"http://localhost:3903/metrics\"] 完事重启Telegraf或者给Telegraf进程发一个SIGHUP信号：\nkill -HUP `pidof telegraf` 等一会，就可以在页面上查到相关指标了，我们拿着mtail_alert_rule_notify_total这个指标去即时查询里查，会发现查不到数据，而是出现了一个mtail_alert_rule_notify_total_counter这样的指标，看起来像是Telegraf对于Prometheus协议的监控数据，自动加了后缀，无所谓了，大家注意一下就好。如果在prometheus.yaml中配置scrape_config来抓取mtail，应该不会自动加上_counter的后缀。\n另外，mtail的配置文件如果发生变化，是需要重启mtail才能生效的，或者也是类似Telegraf那样发一个SIGHUP信号给mtail，mtail收到信号就会重新加载配置。\nmtail更多样例 mtail的github repo中有一个examples，里边有挺多例子，大家可以参考。我在这里再给大家举1个简单例子，比如我们要统计/var/log/messages文件中的 Out of memory 关键字，mtail规则应该怎么写呢？其实比上面举例的mtail_alert_rule_notify_total还要更简单：\ncounter mtail_oom_total /Out of memory/ { mtail_oom_total++ } 关于时间戳 最后说一下时间戳的问题，日志中每一行一般都是有个时间戳的，夜莺v4版本在页面上配置采集规则的时候，就是要选择时间戳的，但是mtail，上面的例子中没有处理时间戳，为啥？其实mtail也可以支持从日志中提取时间戳，如果没有配置的话，就用系统当前时间，个人认为，用系统当前时间就可以了，从日志中提取时间稍微还有点麻烦，当然，系统当前时间和日志中的时间可能稍微有差别，但是不会差很多的，可以接受，examples中的mtail样例，也基本都没有给出时间戳的提取，估计这就是最佳实践。\n",
    "description": "",
    "tags": null,
    "title": "监控日志",
    "uri": "/usage/mtail/"
  },
  {
    "content": " 监控方式：https://github.com/prometheus/mysqld_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/mysql_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/mysql_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控MySQL",
    "uri": "/usage/mysql/"
  },
  {
    "content": " 监控方式：https://github.com/oliver006/redis_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/redis_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/redis_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控Redis",
    "uri": "/usage/redis/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "API",
    "uri": "/api/"
  },
  {
    "content": "  多集群部署的时候MySQL、Redis分别部署几个？   公众号有个视频讲解了 如何接入多个时序存储， 请先查看。MySQL是只需要一个，不管部署多少个n9e-webapi多少个n9e-server。redis也可以只用一个，所有的n9e-webapi和n9e-server共享，但是，如果部署多套n9e-server（一套n9e-server可能是多个n9e-server实例组成一个集群）分布在不同的地域，网络链路可能不太好，此时是建议一套n9e-server对应一个redis，n9e-webapi自己用一个redis\n 架构中可以完全不用Prometheus吗？   原理上是可以的，Prometheus在夜莺的架构中，是作为时序库使用，除了Prometheus，也可以使用VictoriaMetrics、M3DB等，因为这些时序库都实现了Prometheus的Querier接口，而夜莺依赖这些接口拉取监控数据和做告警判断。\n 机器只能归属一个业务组吗？   是的。\n用户可能会继续追问：一台机器可能有混部的情况，同时部署多个服务，那如何来描述这种现实（毕竟，软件就是对现实的建模）呢？其实，这种关联信息在监控中是使用标签来反映的。比如某个监控数据带有这几个标签：host=cs-node001.hna service=n9e-server method=get api=/ping statuscode=200 表示：这个监控数据是n9e-server这个服务的，来自cs-node001.hna这个机器，这条监控数据描述的是/ping接口，/ping接口是个get接口，statuscode是200，这个信息非常丰富，里边既有服务名称，又有机器信息，通过这种方式我们就知道服务和机器的关联关系。\n大家不要把CMDB中的机器分组需求放到夜莺中来维护，这是职责上的错配。\n那为何还要提供业务组这个概念？岂不是多余了？业务组更多的是想处理权限的问题，比如我们的告警规则、屏蔽规则、订阅规则、监控大盘、自愈脚本等，只能由我们自己管理，不能被无关人员修改了或删除了。把这些规则类信息放到某个业务组中，只有这个业务组的管理员有权限管理，其他人就没有权限乱搞了。\n那机器为何要归属业务组？其实也是从权限上考虑的，自愈脚本是归属业务组的，脚本的执行需要有权限控制，不能随便去机器上运行，现在的控制逻辑是脚本只能在自己的业务组下辖的机器上运行。\n综上，坦白讲，我没有想到什么场景是必须让机器（即系统中的监控对象）归属到多个业务组的。关键原因是监控数据上报的时候就是自描述的，已经包含了各类信息了，不需要通过外挂的方式重新归类监控数据，而机器的分组，虽然有需求，但那是CMDB的需求，也不是监控要处理的。\n更新：另外，机器（即系统中的监控对象）可以打标签，可以通过标签体现一定的分类信息，比如region=bj env=prod表示bj区的生产环境的机器。标签是K=V的格式，K可以体现维度信息，大部分归类需求都可以通过标签来解决，唯独比较麻烦的是，在同一个维度有多个值的情况，比如K=V1 K=V2这种情况，这种情况目前不支持，因为标签信息会被附到监控对象的时序数据上，时序数据的标签是map结构，所以K=V1 K=V2这种K相同的情况会产生覆盖，故而页面上监控对象打标签的时候压根就不允许这种标签。\n 为何我的target_up指标一会是0一会是1，导致一会有告警一会又恢复了   这个很可能是因为调整了客户端采集上报频率导致的，默认Telegraf上报频率是10s，不会有这个问题，n9e-server每15s生成一次target_up的值，标识机器是否在正常上报数据，如果发现机器有指标在上报，target_up就置为1，否则就是0，如果把客户端采集上报频率调大，比如改成60s，那n9e-server在某些周期检查的时候，确实就发现客户端没有上报数据，毕竟上报频率太大了。此时可以调整server.conf的配置，把检测客户端是否存活的周期也放大一些，比如调整为60秒：\n[NoData] Metric = \"target_up\" # unit: second Interval = 60 告警规则也可以针对这种情况做一些调整，比如改成：\n# 告警规则的持续时长设置为0，该PromQL表示130s内一直都没有监控数据上报，故而要报警。 max_over_time(target_up[130s]) == 0 target_up指标是0，还有可能的原因是客户端夯住了，可以重启一下客户端试试，或者是客户端所在机器当前压力过大，影响了客户端到服务端的网络通信。\n 夜莺用的redis支持cluster版本或者sentinel版本吗？   不支持，就是用的单机版，改造成cluster版本或者sentinel应该也没啥太大问题，一般公有云会提供高可用的redis，一主一从那种，足够用了，自己搭建也可以，机器挂掉的概率其实很小，满足sla问题不大的\n Telegraf上报的主机标识默认用的机器名，可以让它自动上报IP作为本机标识吗？   可以让它上报IP作为唯一标识，但是要想自动获取IP，做不到。个人建议是使用一些批量执行工具，比如ansible之类的，批量部署Telegraf，部署脚本里自动获取目标机器的IP，然后填充到telegraf.conf中。\n 触发告警和触发恢复的逻辑是什么？   告警规则里有3个配置非常关键，promql、执行频率、持续时长，意思就是按照执行频率，每隔一段时间执行promql查询（即时查询，即调用Prometheus协议的/api/v1/query接口），如果查到数据就认为触发了阈值，触发了阈值是否会产生告警事件，不一定，还要看持续时长，如果持续时长为0，就相当于不用等待，触发了阈值就立马生成事件，如果持续时长大于0，那就要等待，要保证持续时长这段时间内，每次执行promql的查询都触发阈值，才认为应该生成事件。持续时长就相当于prometheus.yml中的alert rule中的for。\n比如promql为 cpu_usage_idle{cpu=\"cpu-total\"} \u003c 20，执行频率是10s，持续时长是60s，就表示在60s内每10s执行一次promql查询，看promql查询是否返回内容，如果6次都返回了，说明应该生成告警事件。\n恢复的逻辑：比如已经产生了告警事件，然后再次拿着promql去查询，发现没有返回内容，那就说明当前的监控数据已经不符合promql中指定的阈值条件了，就表示恢复了。当然， 如果数据丢点了，promql自然也查不到，这种情况也是会报恢复，因为夜莺确实无法区分到底是因为丢点了，还是因为没有满足阈值而导致没有返回内容。那你可能会问，把promql解析一下，去掉promql中的操作符和阈值，只拿着前面部分去查询，不就能区分到底是没数据还是因为没有满足阈值了吗？其实很难，上面举例的promql是一种简单情况，复杂的promql非常复杂，没法这么轻易的拿掉操作符和阈值。\n 如何监控机器的CPU、内存、磁盘、IO、网络、进程？   这个问题，文档里有答案，Telegraf章节，有个调研笔记的链接，调研笔记中描述的很清楚了，除了机器层面的这些监控项，还有讲如何做PING监控，TCP探测等\n 夜莺可以接入到Grafana来展示吗？   可以。但不是把夜莺作为DataSource，因为实在是没必要。夜莺后端可以接入多个时序库：Prometheus、M3DB、VictoriaMetrics等，这些时序库都可以直接作为Grafana的DataSource，所以，只要监控数据进了这些时序库了，Grafana就可以直接展示了\n 夜莺可以配置InfluxDB的QL做告警规则吗？   不支持，当前只支持配置promql，夜莺接入时序库，有两个层面的接入，一是通过remote write，把夜莺收到的数据转发给时序库，所有支持remote write 接口的存储都可以通过这种方式接入夜莺，接收夜莺转发过来的数据；二是时序库如果开放兼容Prometheus的Querier接口，那夜莺还可以读取时序库的数据做告警判断，即Prometheus、M3DB、VictoriaMetrics、Thanos等这些存储，都完全兼容Prometheus的Querier接口，则夜莺可以配置告警规则，从这些存储中读取监控数据做告警判断。而OpenTSDB、InfluxDB等，因为不支持Prometheus的Querier接口，所以夜莺的告警规则，没法读取这些存储的数据做判断，这些存储只能作为remote write的写入端。\n 从哪里获取微信机器人token？   首先，是企业微信，不是微信，在企业微信里建个群，点开群管理，添加机器人，完事查看这个机器人的信息，即可看到webhook地址，webhook地址中包含一个key的参数，key=后面就是token。拿着这个token，去夜莺里创建一个用户，比如就叫xx企微机器人，给这个用户设置一下Wecom Robot Token，配置为刚才从webhook中获取的token，后面把这个人加入告警接收组，相关的告警就会发给企业微信的这个群组。\n 监控对象列表中如何添加监控对象？看到监控对象列表和对象视角都是空的   公众号的视频教程要整体看一遍，就理解整个设计了。简而言之，监控数据需要上报给n9e-server，然后由n9e-server转发给时序库，即监控数据要流经n9e-server，n9e-server才有可能从监控数据中提取ident标签的值，n9e-server会把ident标签的值作为监控对象注册到数据库中，才能在列表中看到。如果监控数据的整个传输过程没有流经n9e-server，或监控数据中没有ident标签，也就没法提取到了。\n夜莺可以支持Telegraf作为客户端采集器，也可以支持grafana-agent作为采集器，这俩采集器实际都没法上报ident标签，但是Telegraf会上报host标签，grafana-agent会上报agent_hostname标签，这俩标签会被自动转换为ident字段。\n当然，即使没有注册到对象视角中，其实也不耽误，没啥大不了的，只是未来看监控图表的时候，没法使用对象视角看图罢了。后面夜莺也会持续优化，增加自定义视图，到时候对象视角的看图可能就可有可无了。\n v5版本如何通过api接口上报数据的时候顺便带上alias字段？   v5目前支持的上报接口是opentsdb的协议、remote write协议、datadog的协议，都不支持alias字段，而数据库中也干掉了这个字段，所以，v5没法通过api上报这个字段了，不过v5有备注字段，和自定义标签字段，可以考虑用这俩字段放置别名，这需要直接操作DB，或者通过API修改监控对象，总之，无法通过上报数据的接口来搞定\n v5邮件告警的服务器配置（smtp）在哪里？   v5.4（含）之后的版本，在server.conf中，v5.4之前的版本，在etc/script/notify.py中，脚本的前面几行，很容易找到\n 我在A业务组配置的告警规则，为何收到了B业务组的机器的告警？   告警规则的逻辑可以查看009号问题，每次就是用promql查询时序库，而时序库里是没有业务组的信息的，所以promql就生效到了全量的监控对象上了。推荐做法：a业务组的机器，统一打上bg=a的标签，b业务组的机器统一打上bg=b的标签，配置告警规则的时候，要带上这个标签做筛选，比如 cpu_usage_idle{bg=\"a\"}\u003c15 就表示只有bg=a这个标签的机器生效，即只有a业务组的机器生效。\n近期夜莺新版本支持在告警规则配置的时候指定只在本业务组生效，一定程度上解决了这个问题，后续也会支持把业务组直接作为标签附到时序数据上，到时候就更方便了\n Telegraf推送数据给n9e-server，如何加上认证机制保证安全？   Telegraf使用datadog的output吐数据给n9e-server（版本要在5.2.1以上）\n[[outputs.datadog]] timeout = \"5s\" url = \"http://localhost:19000/datadog/api/v1/series\" apikey = \"datadog-api-key\" n9e-server中加上这个配置：\n[BasicAuth] apiKey = \"datadog-api-key\" 注意：这样配置之后，表示n9e-server的所有请求都走basic auth认证，原来Telegraf通过openTSDB的接口吐数据的链路就走不通了\n 从低版本怎么向高版本升级？   首先去github的 releases 页面 找到自己的版本，然后挨个查看每个比你高的版本，看看各个版本的release log中写了要更新哪些内容，重点关注sql相关的，比如你的版本是5.1.0，当前版本是5.3.0，那你要看 ( 5.1.0, 5.3.0 ]之间的所有版本，从低到高挨个查看，把每个版本的release log中的sql都执行一遍（如果release log中没有提sql语句，说明这几个版本没有DB表结构变更，那更简单了），最后替换n9e二进制和etc下的内容到最新版本，如果你之前etc下有些特殊配置，记得要基于最新的配置文件再修改下\n telegraf的基本配置和中间件采集的配置都放到一个telegraf.conf中不好管理怎么办？   telegraf启动的时候支持两个参数--config和--config-directory，这俩参数可以一并使用，通过--config指定主配置文件，通过--config-directory指定配置目录，各种非通用的配置，都可以放到配置目录里，每个配置文件以.conf结尾，这样telegraf就可以都识别到了，比如：\n./usr/bin/telegraf --config etc/telegraf/telegraf.conf --config-directory etc/telegraf.d 然后我做了一个mysql.conf放到telegraf.d目录下，专门用于监控mysql，内容如下，供大家参考：\n[[inputs.mysql]] servers = [\"root:1234@tcp(localhost:3306)/?tls=false\"] metric_version = 2 gather_global_variables = true interval_slow = \"1m\" tagexclude = [\"innodb_version\"] Any Questions? 如果问题仍然没有解决，请加微信公众号：cloudmon 公众号底部菜单有各种答疑方式\n",
    "description": "",
    "tags": null,
    "title": "FAQ",
    "uri": "/faq/"
  },
  {
    "content": " Telegraf Windows版本的安装，保姆级教程 - by SL Telegraf Linux版本的安装，保姆级教程 - by SL 弃用Prometheus，搭建单机版本的VictoriaMetrics - by SL 一键部署夜莺到Kubernetes - by 陶柒 使用Telegraf做夜莺5.0的数据采集，样例包含Linux基本信息采集、MySQL、Redis的采集 - by 柴今栋@艾派 telegraf常用中间件采集 修改notify.py为夜莺增加短信通知能力 - by 柴今栋@艾派 使用notify.py接入阿里云语音通知 - by 果 Oracle的简单监控实现 - by 柴今栋@艾派 RocketMQ简单监控的实现 - by 柴今栋@艾派 手把手教你接入钉钉告警 一文说透MySQL监控，使用Prometheus生态的Exporter Vsphere-monitor数据上报夜莺V5监控 使用pg作为数据库替换MySQL  ",
    "description": "",
    "tags": null,
    "title": "社区用户实践分享",
    "uri": "/usecase/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  },
  {
    "content": "夜莺简介  夜莺（ Nightingale ）是一款国产开源、云原生监控系统，Nightingale 在 2020.3.20 发布 v1 版本，目前是 v5 版本，从这个版本开始，与 Prometheus、VictoriaMetrics、Grafana、Telegraf、Datadog 等生态做了协同集成，力争打造国内最好用的开源运维监控系统。出自 Open-Falcon 研发团队。\n 项目代码  后端：💡 https://github.com/didi/nightingale 前端：💡 https://github.com/n9e/fe-v5  前后端都是开源的，如果觉得不错，欢迎 star 一下，给我们持续坚持的动力！\n产品截图 查看监控数据，即监控大盘页面：\n配置告警规则的列表页面：\n活跃告警列表页面，即当前未恢复的告警页面：\n产品架构 Nightingale 有四个核心功能：\n Query Proxy：承接前端时序数据查询请求，转发给时序库，并将时序库返回的结果返回给前端 Push Gateway：承接各类采集客户端的监控数据推送，然后把数据转存到后端多种时序库 Conf Manager：配置管理，比如告警规则、屏蔽规则、订阅规则、自愈脚本、权限等相关配置的管理 Alerting Engine：告警引擎，根据用户配置的 PromQL，查询时序库，判断是否应该触发告警并发送  系统架构 夜莺 v5 的设计非常简单，核心是 server 和 webapi 两个模块，webapi 无状态，放到中心端，承接前端请求，将用户配置写入数据库；server 是告警引擎和数据转发模块，一般随着时序库走，一个时序库就对应一套 server，每套 server 可以只用一个实例，也可以多个实例组成集群，server 可以接收 Telegraf、Grafana-Agent、Datadog-Agent、Falcon-Plugins 上报的数据，写入后端时序库，周期性从数据库同步告警规则，然后查询时序库做告警判断。每套 server 依赖一个 redis。架构图如下：\n产品对比 与 Open-Falcon 的区别 因为开发 Open-Falcon 和 Nightingale 的是一拨人，所以很多社区伙伴会比较好奇，为何要新做一个监控开源软件。核心点是 Open-Falcon 和 Nightingale 的差异点实在是太大了，Nightingale 并非是 Open-Falcon 设计逻辑的一个延续，就看做两个不同的软件就好。\nOpen-Falcon 是 14 年开发的，当时是想解决 Zabbix 的一些容量问题，可以看做是物理机时代的产物，整个设计偏向运维视角，虽然数据结构上已经开始设计了标签，但是查询语法还是比较简单，无法应对比较复杂的场景。\nNightingale 直接支持 PromQL，支持 Prometheus、M3DB、VictoriaMetrics 多种时序库，支持 Telegraf、Datadog-Agent、Grafana-Agent 做监控数据采集，支持 Grafana 看图，整个设计更加云原生。\n与 Prometheus 的区别 Nightingale 可以简单看做是 Prometheus 的一个企业级版本，把 Prometheus 当做 Nightingale 的一个内部组件（时序库），当然，也不是必须的，时序库除了 Prometheus，还可以使用 VictoriaMetrics、M3DB 等，各种 Exporter 采集器也可以继续使用。\nNightingale 可以接入多个 Prometheus，可以允许用户在页面上配置告警规则、屏蔽规则、订阅规则，在页面上查看告警事件、做告警事件聚合统计，配置告警自愈机制，管理监控对象，配置监控大盘等，就把 Nightingale 看做是 Prometheus 的一个 WEBUI 也是可以的，不过实际上，它远远不止是一个 WEBUI，用一下就会深有感触。\n加入社区 微信公众号:cloudmon（云原生监控）这是夜莺的大本营，可以在公众号菜单里找到加群入口、社区答疑入口，关注起来吧！我们团队做运维监控这个事情差不多有 10 年了，一直在坚持，希望能把这个事做到极致，欢迎加入我们一起！\n",
    "description": "",
    "tags": null,
    "title": "夜莺手册",
    "uri": "/"
  }
]
